"""
Disease Prediction Model
H·ªá th·ªëng d·ª± ƒëo√°n b·ªánh d·ª±a tr√™n tri·ªáu ch·ª©ng
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from collections import Counter
import pickle
import json
import os
from datetime import datetime


class DiseasePredictionModel:
    """Class ch√≠nh ƒë·ªÉ x·ª≠ l√Ω d·ª± ƒëo√°n b·ªánh"""
    
    def __init__(self):
        self.model = None
        self.symptoms_list = []
        self.diseases_list = []
        self.df = None
        self.training_history = []
        
    def load_data(self, filepath):
        """
        Load dataset t·ª´ file CSV
        C·∫•u tr√∫c: C·ªôt ƒë·∫ßu ti√™n = t√™n b·ªánh, c√°c c·ªôt sau = tri·ªáu ch·ª©ng (0/1)
        """
        print(f"üìÇ ƒêang load d·ªØ li·ªáu t·ª´: {filepath}")
        
        if not os.path.exists(filepath):
            raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file: {filepath}")
        
        # Load CSV
        self.df = pd.read_csv(filepath)
        
        # L·∫•y danh s√°ch b·ªánh v√† tri·ªáu ch·ª©ng
        self.diseases_list = self.df.iloc[:, 0].unique().tolist()
        self.symptoms_list = self.df.columns[1:].tolist()
        
        print(f"‚úÖ ƒê√£ load th√†nh c√¥ng!")
        print(f"   - S·ªë b·ªánh: {len(self.diseases_list)}")
        print(f"   - S·ªë tri·ªáu ch·ª©ng: {len(self.symptoms_list)}")
        print(f"   - T·ªïng s·ªë m·∫´u: {len(self.df)}")
        
        return self.df
    
    def prepare_data(self):
        """Chu·∫©n b·ªã d·ªØ li·ªáu cho training"""
        if self.df is None:
            raise Exception("Ch∆∞a load d·ªØ li·ªáu! H√£y g·ªçi load_data() tr∆∞·ªõc.")
        
        print("\nüîß ƒêang chu·∫©n b·ªã d·ªØ li·ªáu...")
        
        # X: features (tri·ªáu ch·ª©ng), y: labels (t√™n b·ªánh)
        X = self.df.iloc[:, 1:].values
        y = self.df.iloc[:, 0].values
        
        print(f"‚úÖ D·ªØ li·ªáu ƒë√£ s·∫µn s√†ng!")
        print(f"   - Shape X: {X.shape}")
        print(f"   - Shape y: {y.shape}")
        
        return X, y
    
    def train_model(self, X, y, test_size=0.2, random_state=42):
        """Train model - T·ªëi ∆∞u cho dataset l·ªõn"""
        print("\nüöÄ B·∫Øt ƒë·∫ßu training model...")
        print(f"   - T·ª∑ l·ªá train/test: {int((1-test_size)*100)}/{int(test_size*100)}")
        
        # Ki·ªÉm tra ph√¢n b·ªë class
        class_counts = Counter(y)
        min_samples = min(class_counts.values())
        
        print(f"\nüìä Ph√¢n t√≠ch d·ªØ li·ªáu:")
        print(f"   - T·ªïng s·ªë class (b·ªánh): {len(class_counts)}")
        print(f"   - S·ªë m·∫´u √≠t nh·∫•t: {min_samples}")
        print(f"   - S·ªë m·∫´u nhi·ªÅu nh·∫•t: {max(class_counts.values())}")
        
        # L·ªçc b·ªè class c√≥ √≠t h∆°n 2 m·∫´u
        if min_samples < 2:
            print(f"\n‚ö†Ô∏è  C·∫£nh b√°o: C√≥ {sum(1 for c in class_counts.values() if c < 2)} b·ªánh ch·ªâ c√≥ 1 m·∫´u")
            print("   ‚Üí ƒêang l·ªçc b·ªè c√°c b·ªánh c√≥ qu√° √≠t m·∫´u...")
            
            valid_indices = []
            for i, label in enumerate(y):
                if class_counts[label] >= 2:
                    valid_indices.append(i)
            
            X = X[valid_indices]
            y = y[valid_indices]
            
            print(f"   ‚úì ƒê√£ l·ªçc: {len(X)} m·∫´u c√≤n l·∫°i")
            self.diseases_list = list(set(y))
            class_counts = Counter(y)
        
        # SAMPLING ƒë·ªÉ gi·∫£m dataset
        MAX_SAMPLES_PER_CLASS = 200  # Gi·ªõi h·∫°n m·ªói b·ªánh t·ªëi ƒëa 200 m·∫´u
        
        if max(class_counts.values()) > MAX_SAMPLES_PER_CLASS:
            print(f"\nüîß Dataset qu√° l·ªõn! ƒêang sampling {MAX_SAMPLES_PER_CLASS} m·∫´u/b·ªánh...")
            
            sampled_indices = []
            for disease in set(y):
                disease_indices = np.where(y == disease)[0]
                if len(disease_indices) > MAX_SAMPLES_PER_CLASS:
                    sampled = np.random.choice(disease_indices, MAX_SAMPLES_PER_CLASS, replace=False)
                    sampled_indices.extend(sampled)
                else:
                    sampled_indices.extend(disease_indices)
            
            X = X[sampled_indices]
            y = y[sampled_indices]
            
            print(f"   ‚úì Gi·∫£m xu·ªëng: {len(X)} m·∫´u ({len(set(y))} b·ªánh)")
        
        # ENCODE LABELS
        from sklearn.preprocessing import LabelEncoder
        self.label_encoder = LabelEncoder()
        y_encoded = self.label_encoder.fit_transform(y)
        
        print(f"\nüîß ƒê√£ encode {len(self.label_encoder.classes_)} b·ªánh th√†nh s·ªë 0-{len(self.label_encoder.classes_)-1}")
        
        # Chia d·ªØ li·ªáu
        try:
            X_train, X_test, y_train, y_test = train_test_split(
                X, y_encoded,
                test_size=test_size, 
                random_state=random_state,
                stratify=y_encoded
            )
            
            print(f"\n‚úÖ ƒê√£ chia d·ªØ li·ªáu:")
            print(f"   - S·ªë m·∫´u train: {len(X_train)}")
            print(f"   - S·ªë m·∫´u test: {len(X_test)}")
            
        except ValueError:
            X_train, X_test, y_train, y_test = train_test_split(
                X, y_encoded, 
                test_size=test_size, 
                random_state=random_state,
                stratify=None
            )
        
        # Kh·ªüi t·∫°o XGBoost - T·ªêI ∆ØU
        print("\n‚öôÔ∏è  C·∫•u h√¨nh XGBoost model (t·ªëi ∆∞u):")
        from xgboost import XGBClassifier
        
        self.model = XGBClassifier(
            n_estimators=100,          # Gi·∫£m xu·ªëng 100
            max_depth=8,               # Gi·∫£m ƒë·ªô s√¢u
            learning_rate=0.1,
            subsample=0.8,             # Ch·ªâ d√πng 80% d·ªØ li·ªáu m·ªói tree
            colsample_bytree=0.8,      # Ch·ªâ d√πng 80% features m·ªói tree
            n_jobs=4,                  # Gi·∫£m xu·ªëng 4 cores
            random_state=random_state,
            tree_method='hist',        # Nhanh nh·∫•t
            verbosity=1,
            eval_metric='mlogloss'
        )
        
        print("   - Estimators: 100")
        print("   - Max depth: 8")
        print("   - Subsample: 0.8")
        print("   - Parallel jobs: 4")
        
        # Train v·ªõi progress tracking
        print("\n‚è≥ ƒêang training (∆∞·ªõc t√≠nh 2-5 ph√∫t)...")
        import time
        start_time = time.time()
        
        self.model.fit(
            X_train, y_train,
            eval_set=[(X_test, y_test)],
            verbose=True
        )
        
        training_time = time.time() - start_time
        print(f"\n‚úÖ Training ho√†n t·∫•t! (Th·ªùi gian: {training_time:.1f}s)")
        
        # ƒê√°nh gi√°
        print("\nüìä ƒêang ƒë√°nh gi√° model...")
        
        train_accuracy = self.model.score(X_train, y_train)
        test_accuracy = self.model.score(X_test, y_test)
        
        print("\nüìä K·∫æT QU·∫¢ ƒê√ÅNH GI√Å:")
        print("=" * 50)
        print(f"ƒê·ªô ch√≠nh x√°c tr√™n t·∫≠p TRAIN: {train_accuracy*100:.2f}%")
        print(f"ƒê·ªô ch√≠nh x√°c tr√™n t·∫≠p TEST:  {test_accuracy*100:.2f}%")
        print("=" * 50)
        
        # Metrics
        y_pred = self.model.predict(X_test)
        
        from sklearn.metrics import precision_score, recall_score, f1_score
        
        print("\nüìã Metrics Summary:")
        print(f"   - Accuracy:  {accuracy_score(y_test, y_pred):.4f}")
        print(f"   - Precision: {precision_score(y_test, y_pred, average='weighted', zero_division=0):.4f}")
        print(f"   - Recall:    {recall_score(y_test, y_pred, average='weighted', zero_division=0):.4f}")
        print(f"   - F1-Score:  {f1_score(y_test, y_pred, average='weighted', zero_division=0):.4f}")
        
        # L∆∞u l·ªãch s·ª≠
        self.training_history.append({
            'timestamp': datetime.now().isoformat(),
            'train_accuracy': float(train_accuracy),
            'test_accuracy': float(test_accuracy),
            'n_samples': len(X),
            'test_size': test_size,
            'training_time': training_time
        })
        
        return self.model
    
    def predict_disease(self, selected_symptoms):
            """
            D·ª± ƒëo√°n b·ªánh d·ª±a tr√™n tri·ªáu ch·ª©ng
            """
            if self.model is None:
                raise Exception("Model ch∆∞a ƒë∆∞·ª£c train! H√£y train model tr∆∞·ªõc.")
            
            if not isinstance(selected_symptoms, list):
                raise TypeError("selected_symptoms ph·∫£i l√† list")
            
            if len(selected_symptoms) == 0:
                raise ValueError("Vui l√≤ng ch·ªçn √≠t nh·∫•t 1 tri·ªáu ch·ª©ng")
            
            # T·∫°o vector tri·ªáu ch·ª©ng
            symptom_vector = np.zeros(len(self.symptoms_list))
            
            matched_symptoms = []
            unmatched_symptoms = []
            
            for symptom in selected_symptoms:
                if symptom in self.symptoms_list:
                    idx = self.symptoms_list.index(symptom)
                    symptom_vector[idx] = 1
                    matched_symptoms.append(symptom)
                else:
                    unmatched_symptoms.append(symptom)
            
            # Reshape cho predict
            symptom_vector = symptom_vector.reshape(1, -1)
            
            # D·ª± ƒëo√°n (encoded)
            predicted_encoded = self.model.predict(symptom_vector)[0]
            
            # Decode v·ªÅ t√™n b·ªánh
            if hasattr(self, 'label_encoder'):
                predicted_disease = self.label_encoder.inverse_transform([predicted_encoded])[0]
                probabilities = self.model.predict_proba(symptom_vector)[0]
                
                # Top 5
                top_indices = np.argsort(probabilities)[-5:][::-1]
                
                results = {
                    'primary_prediction': predicted_disease,
                    'confidence': float(probabilities[predicted_encoded]),
                    'matched_symptoms': matched_symptoms,
                    'unmatched_symptoms': unmatched_symptoms,
                    'total_symptoms_checked': len(selected_symptoms),
                    'top_predictions': []
                }
                
                for idx in top_indices:
                    disease_name = self.label_encoder.inverse_transform([idx])[0]
                    probability = probabilities[idx]
                    results['top_predictions'].append({
                        'disease': disease_name,
                        'probability': float(probability),
                        'percentage': f"{float(probability)*100:.1f}%"
                    })
            else:
                # Fallback cho Random Forest (kh√¥ng c·∫ßn encode)
                predicted_disease = self.model.predict(symptom_vector)[0]
                probabilities = self.model.predict_proba(symptom_vector)[0]
                
                top_indices = np.argsort(probabilities)[-5:][::-1]
                
                results = {
                    'primary_prediction': predicted_disease,
                    'confidence': float(probabilities[self.model.classes_.tolist().index(predicted_disease)]),
                    'matched_symptoms': matched_symptoms,
                    'unmatched_symptoms': unmatched_symptoms,
                    'total_symptoms_checked': len(selected_symptoms),
                    'top_predictions': []
                }
                
                for idx in top_indices:
                    disease_name = self.model.classes_[idx]
                    probability = probabilities[idx]
                    results['top_predictions'].append({
                        'disease': disease_name,
                        'probability': float(probability),
                        'percentage': f"{float(probability)*100:.1f}%"
                    })
            
            return results
    
    def get_disease_symptoms(self, disease_name):
        """L·∫•y danh s√°ch tri·ªáu ch·ª©ng c·ªßa m·ªôt b·ªánh"""
        if self.df is None:
            raise Exception("Ch∆∞a load d·ªØ li·ªáu!")
        
        disease_data = self.df[self.df.iloc[:, 0] == disease_name]
        
        if len(disease_data) == 0:
            return []
        
        # L·∫•y c√°c tri·ªáu ch·ª©ng c√≥ gi√° tr·ªã = 1
        symptoms = []
        for col in self.df.columns[1:]:
            if disease_data[col].values[0] == 1:
                symptoms.append(col)
        
        return symptoms
    
    def save_model(self, model_dir='models'):
        """L∆∞u model v√† metadata"""
        print(f"\nüíæ ƒêang l∆∞u model v√†o th∆∞ m·ª•c: {model_dir}")
        
        os.makedirs(model_dir, exist_ok=True)
        
        # L∆∞u model
        model_path = os.path.join(model_dir, 'disease_model.pkl')
        with open(model_path, 'wb') as f:
            pickle.dump(self.model, f)
        print(f"   ‚úì ƒê√£ l∆∞u model: {model_path}")
        
        # L∆∞u label encoder (n·∫øu c√≥)
        if hasattr(self, 'label_encoder'):
            encoder_path = os.path.join(model_dir, 'label_encoder.pkl')
            with open(encoder_path, 'wb') as f:
                pickle.dump(self.label_encoder, f)
            print(f"   ‚úì ƒê√£ l∆∞u label encoder: {encoder_path}")
        
        # L∆∞u danh s√°ch tri·ªáu ch·ª©ng
        symptoms_path = os.path.join(model_dir, 'symptoms_list.json')
        with open(symptoms_path, 'w', encoding='utf-8') as f:
            json.dump(self.symptoms_list, f, ensure_ascii=False, indent=2)
        print(f"   ‚úì ƒê√£ l∆∞u danh s√°ch tri·ªáu ch·ª©ng: {symptoms_path}")
        
        # L∆∞u danh s√°ch b·ªánh
        diseases_path = os.path.join(model_dir, 'diseases_list.json')
        with open(diseases_path, 'w', encoding='utf-8') as f:
            json.dump(self.diseases_list, f, ensure_ascii=False, indent=2)
        print(f"   ‚úì ƒê√£ l∆∞u danh s√°ch b·ªánh: {diseases_path}")
        
        # L∆∞u metadata
        metadata = {
            'model_type': 'XGBoost' if 'XGB' in str(type(self.model)) else 'RandomForest',
            'n_symptoms': len(self.symptoms_list),
            'n_diseases': len(self.diseases_list),
            'training_history': self.training_history,
            'last_trained': datetime.now().isoformat(),
            'has_label_encoder': hasattr(self, 'label_encoder')
        }
        metadata_path = os.path.join(model_dir, 'metadata.json')
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        print(f"   ‚úì ƒê√£ l∆∞u metadata: {metadata_path}")
        
        print("\n‚úÖ L∆∞u model th√†nh c√¥ng!")
    
    def load_model(self, model_dir='models'):
        """Load model ƒë√£ train"""
        print(f"\nüìÇ ƒêang load model t·ª´: {model_dir}")
        
        try:
            # Load model
            model_path = os.path.join(model_dir, 'disease_model.pkl')
            with open(model_path, 'rb') as f:
                self.model = pickle.load(f)
            print(f"   ‚úì ƒê√£ load model")
            
            # Load label encoder (n·∫øu c√≥)
            encoder_path = os.path.join(model_dir, 'label_encoder.pkl')
            if os.path.exists(encoder_path):
                with open(encoder_path, 'rb') as f:
                    self.label_encoder = pickle.load(f)
                print(f"   ‚úì ƒê√£ load label encoder")
            
            # Load symptoms
            symptoms_path = os.path.join(model_dir, 'symptoms_list.json')
            with open(symptoms_path, 'r', encoding='utf-8') as f:
                self.symptoms_list = json.load(f)
            print(f"   ‚úì ƒê√£ load {len(self.symptoms_list)} tri·ªáu ch·ª©ng")
            
            # Load diseases
            diseases_path = os.path.join(model_dir, 'diseases_list.json')
            with open(diseases_path, 'r', encoding='utf-8') as f:
                self.diseases_list = json.load(f)
            print(f"   ‚úì ƒê√£ load {len(self.diseases_list)} b·ªánh")
            
            # Load metadata (optional)
            try:
                metadata_path = os.path.join(model_dir, 'metadata.json')
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                    self.training_history = metadata.get('training_history', [])
                print(f"   ‚úì ƒê√£ load metadata")
            except:
                pass
            
            print("\n‚úÖ Load model th√†nh c√¥ng!")
            
        except FileNotFoundError as e:
            raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file model. Vui l√≤ng train model tr∆∞·ªõc.\nL·ªói: {e}")
        except Exception as e:
            raise Exception(f"L·ªói khi load model: {e}")


if __name__ == "__main__":
    print("Disease Prediction Model Module")
    print("S·ª≠ d·ª•ng class DiseasePredictionModel ƒë·ªÉ train v√† predict")